{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "\n",
    "set_seed(0) # for reproducibility of random modules if used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing:\n",
    "- When used in pipeline, data may come in different formats like json etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tweets = []\n",
    "\n",
    "temp = {}\n",
    "\n",
    "with open(\"data/train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().lower()\n",
    "        arr = line.split('\\t')\n",
    "        if len(arr)==3:\n",
    "            tweets.append(temp)\n",
    "            temp = {}\n",
    "            temp['id'] = arr[1]\n",
    "            if arr[2] == 'negative':\n",
    "                temp['label'] = 2\n",
    "            elif arr[2] == 'positive':\n",
    "                temp['label'] = 1\n",
    "            elif arr[2] == 'neutral':\n",
    "                temp['label'] = 0\n",
    "            else:\n",
    "                print('Error here')\n",
    "                print(arr)\n",
    "                break\n",
    "        elif len(arr)==2:\n",
    "            if 'tokens' in temp:\n",
    "                temp['tokens'].append(arr)\n",
    "            else:\n",
    "                temp['tokens'] = [arr]\n",
    "\n",
    "tweets.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing:\n",
    "- Removal of stopwords related to english and also hinglish, urls, usernames etc followed by tokenizing.\n",
    "- This part mostly comes under the data collection pipeline, for example: collecting the tweets/ feedbacks from different channels and having a service like **AWS Lambda** which processes the text before storing them in a database like **S3**.\n",
    "- Some of the problems which may occur as the pipeline scales is handling all types of texts pertaining to single language, multilingual, Code mixed text etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gampa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "import re\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open(\"data/stop_hinglish.txt\", \"r\") as in_file:\n",
    "    hinglish = in_file.readlines()\n",
    "hinglish = [word.strip() for word in hinglish]\n",
    "union_stopwords = (set(stopwords.words('english')) | set(hinglish))-{'t'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    'urls': '(https?:?(\\s)*\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.+(\\s)*)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', #urls with optional spaces which is the case here\n",
    "    'usernames': '@(\\s)*[\\w]+'\n",
    "}\n",
    "X,x_bert, y = [], [], []\n",
    "for i in range(len(tweets)):\n",
    "    phrase = ''\n",
    "    for word in tweets[i]['tokens']:\n",
    "        word = word[0]\n",
    "        if word.lower() in union_stopwords:\n",
    "            pass\n",
    "        else:\n",
    "            phrase += (' ' + word) if len(phrase) != 0 else word\n",
    "\n",
    "    phrase = re.sub(patterns['urls'], \"\", phrase)\n",
    "    phrase = re.sub(patterns['usernames'],\"\",phrase)\n",
    "    phrase = ' '.join(casual_tokenize(phrase, reduce_len=True, strip_handles=True))\n",
    "    X.append(phrase)\n",
    "    x_bert.append(phrase)\n",
    "    y.append(tweets[i]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- More pre-processing techniques like lemmatisation/normalisation can also be done. Since these are language dependent, we can use the language labels for each word while processing. Some of the problems which are possible are: all languages don't have the support of lemmatisation, specifically the low resource languages. Maybe use language agnostic normalisation techniques like neural stemming or neural lemmatisation.\n",
    "- In practice, we may not have the language labels by default while collecting the data from different channels, we have to have an overhead for language annotations before the preprocessing steps. \n",
    "- If we can have models that achieve good score without some of the preprocessing techniques with high computational overhead, then those pre_processing techniques can be ignored. \n",
    "\n",
    "# Model Training Part:\n",
    "## Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x215f0ee67f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"tweet\"] = X\n",
    "df[\"sentiment\"] = y\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.countplot(x='sentiment', data=df, hue='sentiment') # class distributions, have to take special care when the dataset is imbalanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD7CAYAAACWq8i5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbVUlEQVR4nO3dfXAU9eHH8c+ROyIaLBLvSCamjLVYbGiTabGYiollShIMGdrgICCm9aFTrA0YbWIkMUx8AttIakfD1EqxAnaIiCSk4WgLwoihI420iEZllIAGGo7wYALkklz294fD/eQpfEHubiXv1wzD7fd2s5/c3uVzt3u357AsyxIAAAYGRDoAAOCrg9IAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMackQ4QagcPHlFv77l/FCU2NkZtbR0hSPTl2DEXmczZMReZzNkx14XONGCAQ1dccdkZr7/oS6O31zqv0ji+rB3ZMReZzNkxF5nM2TFXODOxewoAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGLvrPaQB9GXz5IF0SHd6Hgds9WJLU6e9R+2fHwrpu4MuiNNCvXRLtVM6DNRFZ9+qnJ6k9ImsGzh+7pwAAxigNAIAxSgMAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABgLeWk89dRTKi4uliQ1NDQoJydHGRkZqqysDM7T1NSk3NxcZWZmqqSkRD09PZKkPXv26Pbbb1dWVpbuvfdeHTlyJNRxAQB9CGlpbN68Wa+99pokqbOzU3PmzFFVVZXq6+u1fft2bdy4UZJUWFiosrIyrV27VpZlqbq6WpJUXl6u6dOny+v1atSoUaqqqgplXADAWYSsNA4dOqTKykrNnDlTkrRt2zYNHz5ciYmJcjqdysnJkdfrVUtLizo7O5WSkiJJys3NldfrVXd3t7Zs2aLMzMwTxgEAkROy79MoKytTQUGB9u7dK0nat2+f3G538HqPx6PW1tZTxt1ut1pbW3Xw4EHFxMTI6XSeMH6uYmNjzvt3OP5lOXZjx1xkOj92yWiXHF9kx0ySPXOFM1NISuOVV15RfHy8UlNTtXLlSklSb2+vHA5HcB7LsuRwOM44fvz/Lzp52kRbW4d6e61zXs7tHiyfz35fkWPHXF/lTJH+A2CH2+2rvP3CzY65LnSmAQMcfT7ZDklp1NfXy+fzadKkSTp8+LCOHj2qlpYWRUVFBefx+XzyeDyKi4uTz+cLju/fv18ej0dDhw5Ve3u7AoGAoqKigvMDACInJMc0Fi9erLq6OtXU1GjWrFkaN26cXnjhBe3cuVO7du1SIBBQXV2d0tLSlJCQoOjoaDU2NkqSampqlJaWJpfLpdGjR6u+vl6StGrVKqWlpYUiLgDAUNi+Izw6Olrz589Xfn6+/H6/0tPTlZWVJUmqqKhQaWmpOjo6lJSUpLy8PEnS3LlzVVxcrIULFyo+Pl4LFiwIV1wAwGmEvDRyc3OVm5srSUpNTVVtbe0p84wcOVIrVqw4ZTwhIUFLliwJdUQAgCE+EQ4AMEZpAACMURoAAGOUBgDAGKUBADBGaQAAjFEaAABjlAYAwBilAQAwRmkAAIxRGgAAY5QGAMAYpQEAMEZpAACMURoAAGOUBgDAGKUBADBGaQAAjFEaAABjlAYAwBilAQAw5ox0AECSBl8+SJdEX9i7o9s9+IL+PACUBmzikminch6sCft6Vz89KezrBL7K2D0FADBGaQAAjFEaAABjlAYAwBilAQAwRmkAAIxRGgAAY5QGAMAYpQEAMEZpAACMURoAAGOUBgDAGKUBADBGaQAAjHFqdCBCuroDEfnOj05/j9o/Oxb29eLiENLSeOaZZ7R27Vo5HA7deuutuvPOO9XQ0KB58+bJ7/drwoQJKigokCQ1NTWppKRER44c0ejRo1VeXi6n06k9e/aosLBQbW1tuvrqq1VRUaHLLrsslLGBsBjoiorYd4i0h32tuFiEbPfUW2+9pX/961+qra3Vq6++qiVLluj999/XnDlzVFVVpfr6em3fvl0bN26UJBUWFqqsrExr166VZVmqrq6WJJWXl2v69Onyer0aNWqUqqqqQhUZAHAWISuNH/zgB3rppZfkdDrV1tamQCCgzz77TMOHD1diYqKcTqdycnLk9XrV0tKizs5OpaSkSJJyc3Pl9XrV3d2tLVu2KDMz84RxAEBkhPRAuMvl0h/+8AdlZ2crNTVV+/btk9vtDl7v8XjU2tp6yrjb7VZra6sOHjyomJgYOZ3OE8YBAJER8gPhs2bN0i9+8QvNnDlTzc3Ncjgcwessy5LD4VBvb+9px4///0UnT59NbGzMeWePxEFKE3bMZcdMOLOTt5cdt58dM0n2zBXOTCErjY8++khdXV267rrrNGjQIGVkZMjr9SoqKio4j8/nk8fjUVxcnHw+X3B8//798ng8Gjp0qNrb2xUIBBQVFRWc/1y0tXWot9c65/xu92D5fPY7XGjHXBcikx0fiBezL26vi/U+FQp2zHWhMw0Y4OjzyXbIdk99+umnKi0tVVdXl7q6urRu3TpNnTpVO3fu1K5duxQIBFRXV6e0tDQlJCQoOjpajY2NkqSamhqlpaXJ5XJp9OjRqq+vlyStWrVKaWlpoYoMADiLkL3SSE9P17Zt2/STn/xEUVFRysjIUHZ2toYOHar8/Hz5/X6lp6crKytLklRRUaHS0lJ1dHQoKSlJeXl5kqS5c+equLhYCxcuVHx8vBYsWBCqyACAswjpMY38/Hzl5+efMJaamqra2tpT5h05cqRWrFhxynhCQoKWLFkSsowAAHOcRgQAYIzSAAAYMyqNJUuWqKOjI9RZAAA2Z1QaH3zwgTIzM1VSUqJ33nkn1JkAADZldCD88ccfV0dHh1avXq3y8nJZlqVp06YpJydH0dHRoc4IALAJ42MaMTExysrK0sSJE3Xo0CG9/PLLysrK0vr160OZDwBgI0avNDZv3qzly5dr8+bNyszM1HPPPaeRI0dq9+7dmj59usaNGxfqnAAAGzAqjeOnJ3/sscc0ePD/n+7h61//uqZMmRKycAAAezHaPVVbW6shQ4Zo8ODB8vl8evHFF9Xb2yvp8xMSAgD6B6PSeOyxx7Rhw4bPFxgwQI2NjXryySdDmQsAYENGu6e2bt2quro6SVJsbKyeeeYZTZo0KaTBAAD2Y/RKo7u7W11dXcHpnp6ekAUCANiX0SuNm2++WXfffbcmTZokh8Ohuro6paenhzobAMBmjEqjqKhIy5Yt07p16+R0OjV+/HhNnTo11NkAADZjVBpRUVHKy8sLfscFAKB/MiqNf/7zn3ryySd1+PBhWdb/f3Xq22+/HbJgAAD7MSqN3/3udyouLta3v/1tORyOUGcCANiUUWlcfvnlysjICHUWAIDNGb3lNjk5WRs3bgx1FgCAzRm90ti4caOWLl0ql8sll8sly7LkcDg4pgEA/YxRabz44oshjgEA+Cow2j2VkJCgd955R9XV1Ro6dKi2bt2qhISEUGcDANiMUWk8//zz+utf/yqv16vOzk49++yzeu6550KdDQBgM0al8be//U1/+tOfNGjQIF1xxRWqrq4OnsAQANB/GJWG0+nUwIEDg9OXX365nE6jwyEAgIuI0V/++Ph4bdiwQQ6HQ11dXVq0aBHHNACgHzIqjUceeURFRUX64IMPlJKSouTkZFVUVIQ6GwDAZoxKY9iwYfrLX/6iY8eOKRAIKCYmJtS5AAA2ZFQaixcvPu34nXfeeUHDAADszag0Pvzww+Dlrq4ubdmyRampqSELBQCwJ6PSmDdv3gnTra2tKikpCUkgAIB9Gb3l9mTDhg1TS0vLhc4CALC5cz6mYVmWtm/frtjY2JCFAgDY0zkf05A+/9xGUVFRSAIBAOzrvI5pAAD6J6PSuOOOO/r8mteXXnrpggUCANiXUWmMGjVKH330kaZMmSKXy6Wamhr19PQoOzs71PkAADZiVBpvv/22Xn75ZUVFRUmSbrrpJk2ZMkWZmZkhDQcAsBejt9weOHBAfr8/OH3kyBF1dnaGLBQAwJ6MXmlMnDhRt912m8aPHy/LsrRmzRrl5eWddblnn31Wa9askSSlp6erqKhIDQ0Nmjdvnvx+vyZMmKCCggJJUlNTk0pKSnTkyBGNHj1a5eXlcjqd2rNnjwoLC9XW1qarr75aFRUVuuyyy77ErwwAOF9GrzRmz56tWbNm6fDhw/L7/Xr00Uc1ffr0PpdpaGjQpk2b9Nprr2nVqlV69913VVdXpzlz5qiqqkr19fXavn27Nm7cKEkqLCxUWVmZ1q5dK8uyVF1dLUkqLy/X9OnT5fV6NWrUKFVVVX3JXxkAcL6MPxE+bNgwjRgxQvfff79cLtdZ53e73SouLtbAgQPlcrl0zTXXqLm5WcOHD1diYqKcTqdycnLk9XrV0tKizs5OpaSkSJJyc3Pl9XrV3d2tLVu2BI+dHB8HAESG0e6pV199VX/+85/l9/s1fvx4/epXv1JBQYGmTJlyxmVGjBgRvNzc3Kw1a9ZoxowZcrvdwXGPx6PW1lbt27fvhHG3263W1lYdPHhQMTExwW8JPD5+LmJjz/807m734PNeNpTsmMuOmXBmJ28vO24/O2aS7JkrnJmMSmPp0qVavny5ZsyYodjYWK1cuVL33HNPn6Vx3I4dO/TLX/5SRUVFioqKUnNzc/A6y7LkcDjU29t7wudAjo8f//+L+vq8yOm0tXWot9c6p2WkzzeCz9d+zsuFmh1zXYhMdnwgXsy+uL0u1vtUKNgx14XONGCAo88n20a7pwYMGHDCFy/Fx8cH337bl8bGRv385z/Xgw8+qJ/+9KeKi4uTz+cLXu/z+eTxeE4Z379/vzwej4YOHar29nYFAoET5gcARIZRaQwZMkRNTU3BZ/m1tbX62te+1ucye/fu1X333aeKiorghwCTk5O1c+dO7dq1S4FAQHV1dUpLS1NCQoKio6PV2NgoSaqpqVFaWppcLpdGjx6t+vp6SdKqVauUlpZ23r8sAODLMdo9NWfOHM2ePVu7d+/W2LFjFR0dfdZ3MS1atEh+v1/z588Pjk2dOlXz589Xfn6+/H6/0tPTlZWVJUmqqKhQaWmpOjo6lJSUFHxL79y5c1VcXKyFCxcqPj5eCxYsON/fFWcx+PJBuiTa6C5xCnYvAf2D0V+Izs5O1dTUqLm5WYFAQFdfffVZ30FVWlqq0tLS015XW1t7ytjIkSO1YsWKU8YTEhK0ZMkSk5j4ki6JdirnwZqIrHv105Misl4A58Zo99RvfvMbRUVF6ZprrtG1115r9JZbAMDFx6g0vvWtb2n16tXas2ePDh06FPwHAOhfjHZPrVu37pQP1TkcDjU1NYUkFADAnoxK45133gl1DgDAV0Cfu6ceeeSR4OUDBw6EPAwAwN76LI3t27cHL999990hDwMAsLc+S8OyrNNeBgD0T8ZnuT3Xcz4BAC4+fR4I7+3t1eHDh2VZlgKBQPDycUOGDAl5QACAffRZGh9++KFuuOGGYFGMGTMmeB1vuQWA/qfP0nj//ffDlQMA8BVgfEwDAABKAwBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABgLaWl0dHRo4sSJ+vTTTyVJDQ0NysnJUUZGhiorK4PzNTU1KTc3V5mZmSopKVFPT48kac+ePbr99tuVlZWle++9V0eOHAllXADAWYSsNP773/9q2rRpam5uliR1dnZqzpw5qqqqUn19vbZv366NGzdKkgoLC1VWVqa1a9fKsixVV1dLksrLyzV9+nR5vV6NGjVKVVVVoYoLADAQstKorq7W3Llz5fF4JEnbtm3T8OHDlZiYKKfTqZycHHm9XrW0tKizs1MpKSmSpNzcXHm9XnV3d2vLli3KzMw8YRwAEDnOUP3gJ5544oTpffv2ye12B6c9Ho9aW1tPGXe73WptbdXBgwcVExMjp9N5wjiAL6erOyC3e/AJYydPh0Knv0ftnx0L+XoQWiErjZP19vbK4XAEpy3LksPhOOP48f+/6ORpE7GxMeedORwPpPNh11z4ahjoilLOgzVhX+/qpyfpknO479r1fm7HXOHMFLbSiIuLk8/nC077fD55PJ5Txvfv3y+Px6OhQ4eqvb1dgUBAUVFRwfnPVVtbh3p7rXNezu0eLJ+v/ZyXC7VQ5rLjgwEXF9P7bn98/J2vC51pwABHn0+2w1YaycnJ2rlzp3bt2qWrrrpKdXV1mjx5shISEhQdHa3GxkZ9//vfV01NjdLS0uRyuTR69GjV19crJydHq1atUlpaWrjiRtTgywfpkugzbxr+uAOIlLCVRnR0tObPn6/8/Hz5/X6lp6crKytLklRRUaHS0lJ1dHQoKSlJeXl5kqS5c+equLhYCxcuVHx8vBYsWBCuuBF1SbQzYrsPAKAvIS+N9evXBy+npqaqtrb2lHlGjhypFStWnDKekJCgJUuWhDQfAMAcnwgHABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGnJEOAKB/6OoOyO0ebDz/ucx7Np3+HrV/duyC/bz+jNIAEBYDXVHKebAmIute/fQktUdkzRcfSuMMzvVZEQD0B5TGGUT6WREA2BEHwgEAxr4SpbF69WrdcsstysjI0LJlyyIdBwD6LdvvnmptbVVlZaVWrlypgQMHaurUqRozZoy++c1vRjoaAPQ7ti+NhoYG3XDDDRoyZIgkKTMzU16vV7/+9a+Nlh8wwHHe6/ZcMei8l/2yIrVufuf+se7+tt4L+caWc/k5fn+POjo6L8h6+/Jl/s6d689yWJZlXbC1hcAf//hHHT16VAUFBZKkV155Rdu2bdNjjz0W4WQA0P/Y/phGb2+vHI7/bz7Lsk6YBgCEj+1LIy4uTj6fLzjt8/nk8XgimAgA+i/bl8YPf/hDbd68WQcOHNCxY8f097//XWlpaZGOBQD9ku0PhA8bNkwFBQXKy8tTd3e3br31Vn33u9+NdCwA6JdsfyAcAGAftt89BQCwD0oDAGCM0gAAGKM0AADGKI3TsMsJEp999lllZ2crOztbv/3tbyV9flqVnJwcZWRkqLKyMmLZnnrqKRUXF9sm0/r165Wbm6sJEybo8ccft0Wumpqa4PZ76qmnIpqpo6NDEydO1KefftpnjqamJuXm5iozM1MlJSXq6ekJW6bly5dr4sSJysnJ0cMPP6yurq6wZzpdruOWLl2qO+64Izgdydtq69atmjJlirKzs/XAAw+E97aycIL//e9/1o9+9CPr4MGD1pEjR6ycnBxrx44dYc/x5ptvWrfddpvl9/utrq4uKy8vz1q9erWVnp5u7d692+ru7rbuuusua8OGDWHP1tDQYI0ZM8Z66KGHrGPHjkU80+7du62xY8dae/futbq6uqxp06ZZGzZsiGiuo0ePWtdff73V1tZmdXd3W7feequ1bt26iGT6z3/+Y02cONFKSkqyPvnkkz63WXZ2trV161bLsizr4YcftpYtWxaWTB9//LE1fvx4q7293ert7bWKioqsxYsXhzXT6XIdt2PHDuumm26yZsyYERyL1G3V3t5u3XjjjVZTU5NlWZZVUFAQXHc4MvFK4yRfPEHipZdeGjxBYri53W4VFxdr4MCBcrlcuuaaa9Tc3Kzhw4crMTFRTqdTOTk5Yc926NAhVVZWaubMmZKkbdu2RTzTP/7xD91yyy2Ki4uTy+VSZWWlBg0aFNFcgUBAvb29OnbsmHp6etTT06OYmJiIZKqurtbcuXODZ1I40zZraWlRZ2enUlJSJEm5ubkhy3dypoEDB2ru3LmKiYmRw+HQtddeqz179oQ10+lySVJXV5fKyso0a9as4Fgkb6s333xTKSkpGjlypCSptLRU48ePD1sm23+4L9z27dsnt9sdnPZ4PNq2bVvYc4wYMSJ4ubm5WWvWrNGMGTNOydba2hrWXGVlZSooKNDevXslnf72CnemXbt2yeVyaebMmdq7d69uvvlmjRgxIqK5YmJiNHv2bE2YMEGDBg3S9ddfH7Hb6oknnjhh+kw5Th53u90hy3dypoSEBCUkJEiSDhw4oGXLlmnevHlhzXS6XJL09NNPa/LkybrqqquCY5G8rXbt2qVLL71UBQUF+vjjj/W9731PxcXFeu+998KSiVcaJ7HbCRJ37Nihu+66S0VFRUpMTIxotldeeUXx8fFKTU0Njtnh9goEAtq8ebOefPJJLV++XNu2bdMnn3wS0Vzvv/++Xn31Vb3++ut64403NGDAADU3N0f8tpLOvM3ssC1bW1v1s5/9TJMnT9aYMWMinunNN9/U3r17NXny5BPGI5krEAho06ZNeuCBB7Ry5UodO3ZMzz//fNgy8UrjJHFxcfr3v/8dnI7kCRIbGxs1a9YszZkzR9nZ2XrrrbcievLG+vp6+Xw+TZo0SYcPH9bRo0fV0tKiqKioiGWSpCuvvFKpqakaOnSoJOnHP/6xvF5vRHNt2rRJqampio2NlfT5roJFixZF/LaSznwS0JPH9+/fH9Z8H330ke655x7dcccduuuuu06bNdyZ6urqtGPHDk2aNElHjx7V/v37df/996uwsDBiua688kolJycrMTFRkjRhwgQtXbpUubm5YcnEK42T2OUEiXv37tV9992niooKZWdnS5KSk5O1c+dO7dq1S4FAQHV1dWHNtnjxYtXV1ammpkazZs3SuHHj9MILL0Q0kyT96Ec/0qZNm/TZZ58pEAjojTfeUFZWVkRzjRw5Ug0NDTp69Kgsy9L69esjvv2OO1OOhIQERUdHq7GxUdLn7/4KV76Ojg7dfffdmj17drAwJEU0kyTNmzdPa9asUU1NjR5//HGNGjVKv//97yOaa+zYsXr33XeDu4hff/11JSUlhS0TrzROYpcTJC5atEh+v1/z588Pjk2dOlXz589Xfn6+/H6/0tPTlZWVFfZsXxQdHR3xTMnJybrnnns0ffp0dXd368Ybb9S0adP0jW98I2K5xo4dq/fee0+5ublyuVz6zne+o/z8fN14440R3359bbOKigqVlpaqo6NDSUlJysvLC0umFStWaP/+/Vq8eLEWL14sSRo3bpxmz54dsUxnE6lc8fHxevTRRzVz5kz5/X5dd911euihh8KWiRMWAgCMsXsKAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABijNAAAxigNAICx/wMR7ITOR7kflgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length = df['tweet'].str.len().plot.hist(figsize = (6, 4)) # distribution of the tweet lengths, more inference time for large length texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feauture Extraction/ Feature Engineering:\n",
    "- There are different ways of converting the texts to vectors (Or) extracting features from words in the vocabulary like bag of words, tf-idf etc. \n",
    "- Or using pretrained embeddings like word2vec, glove etc. For code mixed data, the vectors corresponding to the two different languages have to be present in a common vector space. For example, fast text has **aligned** vectors [https://fasttext.cc/docs/en/aligned-vectors.html] which has mutlilingual text embeddings in a common vector space. Here **language labels** can be used while converting each word to a vector. \n",
    "- If not for pretrained, we can train our own embeddings through an embedding layer if neural networks are used Or fit a custom word2vec on the data vocabulary we have. Drawbacks: poor representations if the data is less. Maybe using pretrained multilingual versions of BERT or GPT is a better solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13617, 2500)\n",
      "(1513, 2500)\n",
      "(13617,)\n",
      "(1513,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features = 2500)\n",
    "x = cv.fit_transform(X).toarray()\n",
    "y = np.array(y)\n",
    "# Splitting the data sets\n",
    "test_size = 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=42) #K fold cv is robust to the splitting of dataset\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training:\n",
    "- Once we have the preprocessed data, we need to decide the model architectures. Sometimes, it is also possible that we may need to use ensemble of different architectures. This also involves hyperparamter tuning etc. This process is mostly done offline.\n",
    "- And it is also good to do different ablations corresponding to different preprocessing techniques. For example, if the final accuracy of the best model doesn't differ much when one preprocessing tecchnique is removed, we can ignore this for improving the overall latency.\n",
    "- While I have used non deep learning based architectures in this notebook, deep learning architectures like RNN, CNN can also be used with softmax layer at the output with three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for RandomForest\n",
      "Training time: 123.279668s; Prediction time: 0.248341s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.51      0.50       587\n",
      "           1       0.61      0.60      0.61       486\n",
      "           2       0.59      0.59      0.59       440\n",
      "\n",
      "    accuracy                           0.56      1513\n",
      "   macro avg       0.57      0.57      0.57      1513\n",
      "weighted avg       0.56      0.56      0.56      1513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import time\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "t2 = time.time()\n",
    "time_train = t1-t0\n",
    "time_predict = t2-t1\n",
    "\n",
    "\n",
    "print(\"Results for RandomForest\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_train, time_predict))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for DecisionTree\n",
      "Training time: 80.127704s; Prediction time: 0.015960s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.50      0.48       587\n",
      "           1       0.55      0.52      0.53       486\n",
      "           2       0.53      0.52      0.52       440\n",
      "\n",
      "    accuracy                           0.51      1513\n",
      "   macro avg       0.52      0.51      0.51      1513\n",
      "weighted avg       0.51      0.51      0.51      1513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "t2 = time.time()\n",
    "time_train = t1-t0\n",
    "time_predict = t2-t1\n",
    "\n",
    "\n",
    "print(\"Results for DecisionTree\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_train, time_predict))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SVC\n",
      "Training time: 689.107048s; Prediction time: 62.861432s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.63      0.56       587\n",
      "           1       0.68      0.56      0.61       486\n",
      "           2       0.62      0.54      0.58       440\n",
      "\n",
      "    accuracy                           0.58      1513\n",
      "   macro avg       0.60      0.58      0.59      1513\n",
      "weighted avg       0.60      0.58      0.58      1513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "t2 = time.time()\n",
    "time_train = t1-t0\n",
    "time_predict = t2-t1\n",
    "\n",
    "\n",
    "print(\"Results for SVC\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_train, time_predict))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for XGBClassifier\n",
      "Training time: 179.602680s; Prediction time: 0.134640s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.69      0.60       587\n",
      "           1       0.69      0.60      0.64       486\n",
      "           2       0.66      0.50      0.57       440\n",
      "\n",
      "    accuracy                           0.60      1513\n",
      "   macro avg       0.63      0.60      0.60      1513\n",
      "weighted avg       0.62      0.60      0.60      1513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "t2 = time.time()\n",
    "time_train = t1-t0\n",
    "time_predict = t2-t1\n",
    "\n",
    "\n",
    "print(\"Results for XGBClassifier\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_train, time_predict))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT: BERT with less parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 13\n",
      "\t95percentile : 21\n",
      "\t99percentile : 25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c3fdf1c38e470aa49000d7d19a8cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 13\n",
      "\t95percentile : 21\n",
      "\t99percentile : 24\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c716d9fbb44427b829a7a2067caaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64ae9807d7e4389a1e9d64b91fc4566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import ktrain and the ktrain.text modules, ktrain is a wrapper over keras which can be used for fast prototyping of transformer models\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "\n",
    "# Splitting the data sets\n",
    "test_size = 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_bert, np.array(y), test_size=test_size, random_state=42) #K fold cv is robust to the splitting of dataset\n",
    "\n",
    "\n",
    "#preprocess the data for BERT i.e using bert's vocabulary\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "t = text.Transformer(MODEL_NAME, maxlen=150, class_names=['neutral','positive','negative'])\n",
    "trn = t.preprocess_train(X_train, y_train)\n",
    "val = t.preprocess_test(X_test, y_test)\n",
    "model = t.get_classifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 2e-05...\n",
      "2270/2270 [==============================] - 6349s 3s/step - loss: 0.9155 - accuracy: 0.5535 - val_loss: 0.8242 - val_accuracy: 0.6147\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.58      0.48      0.53       587\n",
      "    positive       0.68      0.68      0.68       486\n",
      "    negative       0.59      0.72      0.65       440\n",
      "\n",
      "    accuracy                           0.61      1513\n",
      "   macro avg       0.62      0.63      0.62      1513\n",
      "weighted avg       0.61      0.61      0.61      1513\n",
      "\n",
      "Results for BERT\n",
      "Training time: 6366.899460s; Prediction time: 182.069712s\n"
     ]
    }
   ],
   "source": [
    "#model = text.text_classifier('bert', train_data=(x_train, y_train), preproc=preproc)\n",
    "#learner = ktrain.get_learner(model, train_data=(x_train, y_train), batch_size=6)\n",
    "learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)\n",
    "\n",
    "t0 = time.time()\n",
    "learner.fit_onecycle(2e-5, 1)    # finetune the model for 1 epochs with lr 2e-5\n",
    "t1 = time.time()\n",
    "learner.validate(class_names=['neutral','positive','negative'])\n",
    "t2 = time.time()\n",
    "time_train = t1-t0\n",
    "time_predict = t2-t1\n",
    "\n",
    "# only using pretrained model\n",
    "print(\"Results for BERT\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_train, time_predict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the above models, xgboost has the best scores and the second best latency in terms of prediction time while decision tree has the best performance (8 times better speed but 1.2 times less accuracy than xgboost).\n",
    "- Assuming we don't have fast implementation of xgboost like in c++, decision tree can be used for achieving better throughput.\n",
    "- But, xgboost (lot of decision trees) has better generalisation capabilities than simple decision tree. Maybe perform A/B testing to decide? because we don't know whether incoming data is really different than the one trained upon. Ensemble models generally have advantages regarding generalisability.\n",
    "- And for DistillBert, as expected, it takes lot of time for inference. But, BERT has advantages when compared to all other models as it has the capability of zero-shot generalisation. (Achieving resonable performance on entirely new data or language. ex: mBERT is a multilingual bert which is trained on one language but can be used for multi languages directly) \n",
    "\n",
    "\n",
    "# Model Serving:\n",
    "- Have to decide whether the inference is batch_wise or realtime.\n",
    "- If we have done enough ablation studies during model training time, some decisions have to be made on different stages of pipelines for improving the overall latency starting from the data preprocessing stage. There can always be a latency vs accuracy tradeoff. \n",
    "- Non realtime use cases have the biggest advantage in that we can perform the inference with best model offline before hand and periodically update as the new data comes in, and serve it directly when requested. (having better latency and accuracy)\n",
    "- Since the model also has to relearn as the new data comes in, we have to decided how frequently it has to be updated.\n",
    "- At each stage, we may decide on modifications to the other stages in the hierarchy for improving the overall latency.\n",
    "- For having fast inference, maybe writing some components in c++ can be done. Like [https://github.com/LieluoboAi/radish/blob/master/radish/bert/bert_tokenizer.h] has tokenisation code for Bert written in C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
