{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing Glove Vectors\n",
    "- The Glove Vectors are post processed according to the algorithm given in the paper https://arxiv.org/pdf/1702.01417.pdf\n",
    "- Several benchmark datasets from https://github.com/kudkudak/word-embeddings-benchmarks are used to check the performance of the processsed vectors.\n",
    "- The datasets are related to different tasks of similarity, analogy and categorisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from web.analogy import fetch_google_analogy, fetch_msr_analogy\n",
    "from web.embedding import Embedding\n",
    "from web.evaluate import evaluate_analogy, evaluate_on_all\n",
    "from web.vocabulary import Vocabulary\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(2020) # for reproducibility of random modules if used\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(zip_path, dim=300):\n",
    "    words = []\n",
    "    embedding_matrix = []\n",
    "    with zipfile.ZipFile(zip_path) as zip_file:\n",
    "        with zip_file.open('glove.6B.{}d.txt'.format(dim), 'r') as file:\n",
    "            file = io.TextIOWrapper(file)\n",
    "            for line in file:\n",
    "                line_split = line.split()\n",
    "                word, vector = line_split[0], np.array(line_split[1:], dtype='float64')\n",
    "                words.append(word)\n",
    "                embedding_matrix.append(vector)\n",
    "    embedding_matrix = np.stack(embedding_matrix)\n",
    "    return words, embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm to process the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_embeddings(embedding_matrix, D, inplace=False):\n",
    "    if not inplace:\n",
    "        embedding_matrix = embedding_matrix.copy()\n",
    "    embedding_matrix = embedding_matrix - embedding_matrix.mean(axis=0)\n",
    "    \n",
    "    pca = PCA()\n",
    "    pca = pca.fit(embedding_matrix)\n",
    "    #I think this is  the step where the algorithm has typos. We need to use v_bar instead of v \n",
    "    coeffs = np.matmul(embedding_matrix, pca.components_[:D].T)\n",
    "    embedding_matrix = embedding_matrix - np.matmul(coeffs, pca.components_[:D])\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the performance of both embeddings on different tasks\n",
    "- Categorisation tasks : **AP** , **BLESS** ,**Battig** ,**ESSLI_2c**, **ESSLI_2b**, **ESSLI_1a**\n",
    "- Analogy tasks : **Google**, **MSR**, **SemEval2012_2**\n",
    "- Word Similarity tasks: **MEN**, **WS353**, **WS353R**, **WS353S**, **SimLex999**, **RW**, **RG65**, **MTurk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " words, embedding_matrix = load_glove_embeddings(zip_path='glove.6B.zip', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:336: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  A = np.vstack(w.get(word, mean_vector) for word in X[:, 0])\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:337: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  B = np.vstack(w.get(word, mean_vector) for word in X[:, 1])\n",
      "Missing 24 words. Will replace them with mean vector\n",
      "Missing 16 words. Will replace them with mean vector\n",
      "Missing 11 words. Will replace them with mean vector\n",
      "Missing 260 words. Will replace them with mean vector\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/analogy.py:105: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  A, B, C = np.vstack(w.get(word, mean_vector) for word in X_b[:, 0]), \\\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/analogy.py:106: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  np.vstack(w.get(word, mean_vector) for word in X_b[:, 1]), \\\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/analogy.py:107: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  np.vstack(w.get(word, mean_vector) for word in X_b[:, 2])\n",
      "Missing 164 words. Will replace them with mean vector\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:143: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  prot_left = np.mean(np.vstack(w.get(word, mean_vector) for word in prototypes[:, 0]), axis=0)\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:144: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  prot_right = np.mean(np.vstack(w.get(word, mean_vector) for word in prototypes[:, 1]), axis=0)\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:147: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  question_left, question_right = np.vstack(w.get(word, mean_vector) for word in questions[:, 0]), \\\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:148: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  np.vstack(w.get(word, mean_vector) for word in questions[:, 1])\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:88: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  words = np.vstack(w.get(word, mean_vector) for word in X.flatten())\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:88: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  words = np.vstack(w.get(word, mean_vector) for word in X.flatten())\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:88: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  words = np.vstack(w.get(word, mean_vector) for word in X.flatten())\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:88: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  words = np.vstack(w.get(word, mean_vector) for word in X.flatten())\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:88: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  words = np.vstack(w.get(word, mean_vector) for word in X.flatten())\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:88: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  words = np.vstack(w.get(word, mean_vector) for word in X.flatten())\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings = Embedding(vocabulary=Vocabulary(words), vectors=embedding_matrix)\n",
    "glove_analogy_results = evaluate_on_all(glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_embedding_matrix = process_embeddings(embedding_matrix, D=2, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:336: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  A = np.vstack(w.get(word, mean_vector) for word in X[:, 0])\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:337: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  B = np.vstack(w.get(word, mean_vector) for word in X[:, 1])\n",
      "Missing 24 words. Will replace them with mean vector\n",
      "Missing 16 words. Will replace them with mean vector\n",
      "Missing 11 words. Will replace them with mean vector\n",
      "Missing 260 words. Will replace them with mean vector\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/analogy.py:105: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  A, B, C = np.vstack(w.get(word, mean_vector) for word in X_b[:, 0]), \\\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/analogy.py:106: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  np.vstack(w.get(word, mean_vector) for word in X_b[:, 1]), \\\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/analogy.py:107: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  np.vstack(w.get(word, mean_vector) for word in X_b[:, 2])\n",
      "Missing 164 words. Will replace them with mean vector\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:143: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  prot_left = np.mean(np.vstack(w.get(word, mean_vector) for word in prototypes[:, 0]), axis=0)\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:144: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  prot_right = np.mean(np.vstack(w.get(word, mean_vector) for word in prototypes[:, 1]), axis=0)\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:147: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  question_left, question_right = np.vstack(w.get(word, mean_vector) for word in questions[:, 0]), \\\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:148: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  np.vstack(w.get(word, mean_vector) for word in questions[:, 1])\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:88: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  words = np.vstack(w.get(word, mean_vector) for word in X.flatten())\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:88: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  words = np.vstack(w.get(word, mean_vector) for word in X.flatten())\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:88: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  words = np.vstack(w.get(word, mean_vector) for word in X.flatten())\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:88: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  words = np.vstack(w.get(word, mean_vector) for word in X.flatten())\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:88: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  words = np.vstack(w.get(word, mean_vector) for word in X.flatten())\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/evaluate.py:88: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  words = np.vstack(w.get(word, mean_vector) for word in X.flatten())\n"
     ]
    }
   ],
   "source": [
    "processed_embeddings = Embedding(vocabulary=Vocabulary(words), vectors=processed_embedding_matrix)\n",
    "processed_analogy_results = evaluate_on_all(processed_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AP</th>\n",
       "      <th>BLESS</th>\n",
       "      <th>Battig</th>\n",
       "      <th>ESSLI_2c</th>\n",
       "      <th>ESSLI_2b</th>\n",
       "      <th>ESSLI_1a</th>\n",
       "      <th>MEN</th>\n",
       "      <th>WS353</th>\n",
       "      <th>WS353R</th>\n",
       "      <th>WS353S</th>\n",
       "      <th>SimLex999</th>\n",
       "      <th>RW</th>\n",
       "      <th>RG65</th>\n",
       "      <th>MTurk</th>\n",
       "      <th>Google</th>\n",
       "      <th>MSR</th>\n",
       "      <th>SemEval2012_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.629353</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.408144</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.737465</td>\n",
       "      <td>0.543326</td>\n",
       "      <td>0.477487</td>\n",
       "      <td>0.661995</td>\n",
       "      <td>0.3705</td>\n",
       "      <td>0.367045</td>\n",
       "      <td>0.769525</td>\n",
       "      <td>0.633182</td>\n",
       "      <td>0.650993</td>\n",
       "      <td>0.514125</td>\n",
       "      <td>0.151804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AP  BLESS    Battig  ESSLI_2c  ESSLI_2b  ESSLI_1a       MEN  \\\n",
       "0  0.629353  0.795  0.408144       0.6     0.825      0.75  0.737465   \n",
       "\n",
       "      WS353    WS353R    WS353S  SimLex999        RW      RG65     MTurk  \\\n",
       "0  0.543326  0.477487  0.661995     0.3705  0.367045  0.769525  0.633182   \n",
       "\n",
       "     Google       MSR  SemEval2012_2  \n",
       "0  0.650993  0.514125       0.151804  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_analogy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AP</th>\n",
       "      <th>BLESS</th>\n",
       "      <th>Battig</th>\n",
       "      <th>ESSLI_2c</th>\n",
       "      <th>ESSLI_2b</th>\n",
       "      <th>ESSLI_1a</th>\n",
       "      <th>MEN</th>\n",
       "      <th>WS353</th>\n",
       "      <th>WS353R</th>\n",
       "      <th>WS353S</th>\n",
       "      <th>SimLex999</th>\n",
       "      <th>RW</th>\n",
       "      <th>RG65</th>\n",
       "      <th>MTurk</th>\n",
       "      <th>Google</th>\n",
       "      <th>MSR</th>\n",
       "      <th>SemEval2012_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.614428</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.416555</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.754177</td>\n",
       "      <td>0.571465</td>\n",
       "      <td>0.511425</td>\n",
       "      <td>0.679432</td>\n",
       "      <td>0.389328</td>\n",
       "      <td>0.343098</td>\n",
       "      <td>0.765722</td>\n",
       "      <td>0.641538</td>\n",
       "      <td>0.641066</td>\n",
       "      <td>0.4745</td>\n",
       "      <td>0.144469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AP  BLESS    Battig  ESSLI_2c  ESSLI_2b  ESSLI_1a       MEN  \\\n",
       "0  0.614428   0.78  0.416555       0.6      0.75      0.75  0.754177   \n",
       "\n",
       "      WS353    WS353R    WS353S  SimLex999        RW      RG65     MTurk  \\\n",
       "0  0.571465  0.511425  0.679432   0.389328  0.343098  0.765722  0.641538   \n",
       "\n",
       "     Google     MSR  SemEval2012_2  \n",
       "0  0.641066  0.4745       0.144469  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_analogy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "- For the analogy tasks, **original** word embeddings perform better than the processed vectors. (Maybe because glove embeddings are trained using the co-occurance ratios directly which is highly similar to analogy task)\n",
    "- For the similarity tasks, **processed** word embeddings perform better than the original vectors.\n",
    "- For the categorisation tasks, **original** word embeddings perform better than the processed vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pca1 = PCA().fit(embedding_matrix)\n",
    "pca2 = PCA().fit(processed_embedding_matrix)\n",
    "\n",
    "singular1 = preprocessing.normalize(pca1.singular_values_.reshape(1,-1))\n",
    "singular2 = preprocessing.normalize(pca2.singular_values_.reshape(1,-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Normalised Singular Values')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(singular1[0], label=\"Glove\")\n",
    "plt.plot(singular2[0], label=\"Processed Glove\")\n",
    "plt.legend()\n",
    "plt.title(\"Normalised Singular Values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the processed vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_write = open(\"processed_embeddings.npz\",\"wb\")\n",
    "np.savez(file_to_write, words = np.array(words), embeddings = np.array(processed_embedding_matrix))\n",
    "file_to_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples from a task\n",
    "## Similarity task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gampapha/.conda/envs/nlmatics_2/lib/python3.6/site-packages/ipykernel_launcher.py:38: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "/home/mila/g/gampapha/.conda/envs/nlmatics_2/lib/python3.6/site-packages/ipykernel_launcher.py:39: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999, fetch_MTurk, fetch_RG65, fetch_RW, fetch_TR9856\n",
    "import scipy\n",
    "def evaluate_similarity(w, X, y):\n",
    "    \"\"\"\n",
    "    Calculate Spearman correlation between cosine similarity of the model\n",
    "    and human rated similarity of word pairs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : Embedding or dict\n",
    "      Embedding or dict instance.\n",
    "\n",
    "    X: array, shape: (n_samples, 2)\n",
    "      Word pairs\n",
    "\n",
    "    y: vector, shape: (n_samples,)\n",
    "      Human ratings\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cor: float\n",
    "      Spearman correlation\n",
    "    \"\"\"\n",
    "    if isinstance(w, dict):\n",
    "        w = Embedding.from_dict(w)\n",
    "\n",
    "    missing_words = 0\n",
    "    words = w.vocabulary.word_id\n",
    "    for query in X:\n",
    "        for query_word in query:\n",
    "            if query_word not in words:\n",
    "                missing_words += 1\n",
    "    if missing_words > 0:\n",
    "        logger.warning(\"Missing {} words. Will replace them with mean vector\".format(missing_words))\n",
    "\n",
    "\n",
    "    mean_vector = np.mean(w.vectors, axis=0, keepdims=True)\n",
    "    A = np.vstack(w.get(word, mean_vector) for word in X[:, 0])\n",
    "    B = np.vstack(w.get(word, mean_vector) for word in X[:, 1])\n",
    "    scores = np.array([v1.dot(v2.T)/(np.linalg.norm(v1)*np.linalg.norm(v2)) for v1, v2 in zip(A, B)])\n",
    "    return scores, scipy.stats.spearmanr(scores, y).correlation\n",
    "\n",
    "data= fetch_MEN()\n",
    "scores_glove, _ = evaluate_similarity(glove_embeddings,data.X,data.y)\n",
    "scores_processed, _ = evaluate_similarity(processed_embeddings,data.X,data.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_k_scores</th>\n",
       "      <th>min_k_words</th>\n",
       "      <th>min_k_true</th>\n",
       "      <th>max_k_scores</th>\n",
       "      <th>max_k_words</th>\n",
       "      <th>max_k_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.144221</td>\n",
       "      <td>construction&lt;-&gt;violet</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.721251</td>\n",
       "      <td>dinner&lt;-&gt;lunch</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.127091</td>\n",
       "      <td>carrot&lt;-&gt;city</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.721868</td>\n",
       "      <td>breakfast&lt;-&gt;dinner</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.126579</td>\n",
       "      <td>people&lt;-&gt;stair</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.724834</td>\n",
       "      <td>autumn&lt;-&gt;spring</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.123640</td>\n",
       "      <td>jellyfish&lt;-&gt;rally</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.725456</td>\n",
       "      <td>rail&lt;-&gt;railway</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.122103</td>\n",
       "      <td>bay&lt;-&gt;chipmunk</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.727916</td>\n",
       "      <td>game&lt;-&gt;play</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.119800</td>\n",
       "      <td>bracelet&lt;-&gt;mountain</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.729003</td>\n",
       "      <td>aircraft&lt;-&gt;airplane</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.117212</td>\n",
       "      <td>lego&lt;-&gt;weather</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.732590</td>\n",
       "      <td>cafe&lt;-&gt;restaurant</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.114480</td>\n",
       "      <td>explosion&lt;-&gt;stencil</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.745041</td>\n",
       "      <td>shop&lt;-&gt;store</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.108955</td>\n",
       "      <td>graveyard&lt;-&gt;silver</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.752356</td>\n",
       "      <td>cattle&lt;-&gt;sheep</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.108011</td>\n",
       "      <td>clown&lt;-&gt;flood</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.761904</td>\n",
       "      <td>evening&lt;-&gt;night</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.103186</td>\n",
       "      <td>seagull&lt;-&gt;urban</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.763093</td>\n",
       "      <td>beef&lt;-&gt;meat</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.102684</td>\n",
       "      <td>food&lt;-&gt;gull</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.765537</td>\n",
       "      <td>car&lt;-&gt;vehicle</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.095870</td>\n",
       "      <td>origami&lt;-&gt;stadium</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.773739</td>\n",
       "      <td>airplane&lt;-&gt;plane</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.095777</td>\n",
       "      <td>skating&lt;-&gt;sticker</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.786959</td>\n",
       "      <td>evening&lt;-&gt;morning</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.090772</td>\n",
       "      <td>building&lt;-&gt;zombie</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.794049</td>\n",
       "      <td>summer&lt;-&gt;winter</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.090095</td>\n",
       "      <td>mammal&lt;-&gt;write</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.795643</td>\n",
       "      <td>daughter&lt;-&gt;son</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.090094</td>\n",
       "      <td>feel&lt;-&gt;stair</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.826195</td>\n",
       "      <td>bicycle&lt;-&gt;bike</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.089189</td>\n",
       "      <td>pattern&lt;-&gt;pier</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.827289</td>\n",
       "      <td>boy&lt;-&gt;girl</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.088808</td>\n",
       "      <td>dandelion&lt;-&gt;stand</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.831060</td>\n",
       "      <td>daughter&lt;-&gt;mother</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.087140</td>\n",
       "      <td>portrait&lt;-&gt;rocket</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.848094</td>\n",
       "      <td>gold&lt;-&gt;silver</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    min_k_scores            min_k_words  min_k_true  max_k_scores  \\\n",
       "0      -0.144221  construction<->violet         0.6      0.721251   \n",
       "1      -0.127091          carrot<->city         1.0      0.721868   \n",
       "2      -0.126579         people<->stair         4.2      0.724834   \n",
       "3      -0.123640      jellyfish<->rally         0.4      0.725456   \n",
       "4      -0.122103         bay<->chipmunk         1.4      0.727916   \n",
       "5      -0.119800    bracelet<->mountain         0.8      0.729003   \n",
       "6      -0.117212         lego<->weather         0.8      0.732590   \n",
       "7      -0.114480    explosion<->stencil         0.6      0.745041   \n",
       "8      -0.108955     graveyard<->silver         3.0      0.752356   \n",
       "9      -0.108011          clown<->flood         1.2      0.761904   \n",
       "10     -0.103186        seagull<->urban         2.8      0.763093   \n",
       "11     -0.102684            food<->gull         4.0      0.765537   \n",
       "12     -0.095870      origami<->stadium         1.2      0.773739   \n",
       "13     -0.095777      skating<->sticker         2.2      0.786959   \n",
       "14     -0.090772      building<->zombie         1.4      0.794049   \n",
       "15     -0.090095         mammal<->write         1.0      0.795643   \n",
       "16     -0.090094           feel<->stair         3.4      0.826195   \n",
       "17     -0.089189         pattern<->pier         2.6      0.827289   \n",
       "18     -0.088808      dandelion<->stand         1.2      0.831060   \n",
       "19     -0.087140      portrait<->rocket         2.0      0.848094   \n",
       "\n",
       "            max_k_words  max_k_true  \n",
       "0        dinner<->lunch         7.4  \n",
       "1    breakfast<->dinner         7.4  \n",
       "2       autumn<->spring         8.8  \n",
       "3        rail<->railway         8.6  \n",
       "4           game<->play         6.4  \n",
       "5   aircraft<->airplane         9.2  \n",
       "6     cafe<->restaurant         8.4  \n",
       "7          shop<->store         7.8  \n",
       "8        cattle<->sheep         7.6  \n",
       "9       evening<->night         8.4  \n",
       "10          beef<->meat         8.6  \n",
       "11        car<->vehicle         9.2  \n",
       "12     airplane<->plane         8.4  \n",
       "13    evening<->morning         6.4  \n",
       "14      summer<->winter         7.4  \n",
       "15       daughter<->son         8.2  \n",
       "16       bicycle<->bike         9.0  \n",
       "17           boy<->girl         7.0  \n",
       "18    daughter<->mother         8.0  \n",
       "19        gold<->silver         8.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "num_words = 20\n",
    "min_ten = np.argsort(scores_glove)[:num_words]\n",
    "max_ten= np.argsort(scores_glove)[-num_words:]\n",
    "\n",
    "df1[\"min_k_scores\"] = pd.Series(scores_glove[min_ten])\n",
    "df1[\"min_k_words\"] = pd.Series([\"<->\".join((a[0],a[1])) for a in data.X[min_ten]])\n",
    "df1[\"min_k_true\"] = pd.Series([a[0] for a in data.y[min_ten]])\n",
    "df1[\"max_k_scores\"] = pd.Series(scores_glove[max_ten])\n",
    "df1[\"max_k_words\"] = pd.Series([\"<->\".join((a[0],a[1])) for a in data.X[max_ten]])\n",
    "df1[\"max_k_true\"] = pd.Series([a[0] for a in data.y[max_ten]])\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processed Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_k_scores</th>\n",
       "      <th>min_k_words</th>\n",
       "      <th>min_k_true</th>\n",
       "      <th>max_k_scores</th>\n",
       "      <th>max_k_words</th>\n",
       "      <th>max_k_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123695</td>\n",
       "      <td>carrot&lt;-&gt;city</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.718475</td>\n",
       "      <td>town&lt;-&gt;village</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.116446</td>\n",
       "      <td>construction&lt;-&gt;violet</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.726054</td>\n",
       "      <td>dinner&lt;-&gt;lunch</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.109061</td>\n",
       "      <td>jellyfish&lt;-&gt;rally</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.727312</td>\n",
       "      <td>breakfast&lt;-&gt;dinner</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.107640</td>\n",
       "      <td>bracelet&lt;-&gt;mountain</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.731430</td>\n",
       "      <td>rail&lt;-&gt;railway</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.102209</td>\n",
       "      <td>people&lt;-&gt;stair</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.737399</td>\n",
       "      <td>autumn&lt;-&gt;spring</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.094024</td>\n",
       "      <td>skating&lt;-&gt;sticker</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.737639</td>\n",
       "      <td>aircraft&lt;-&gt;airplane</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.093932</td>\n",
       "      <td>lego&lt;-&gt;weather</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.743298</td>\n",
       "      <td>cafe&lt;-&gt;restaurant</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.090030</td>\n",
       "      <td>daisy&lt;-&gt;gravestone</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.748254</td>\n",
       "      <td>shop&lt;-&gt;store</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.088359</td>\n",
       "      <td>origami&lt;-&gt;stadium</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.753521</td>\n",
       "      <td>daughter&lt;-&gt;son</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.082827</td>\n",
       "      <td>graveyard&lt;-&gt;silver</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.755752</td>\n",
       "      <td>cattle&lt;-&gt;sheep</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.081571</td>\n",
       "      <td>clown&lt;-&gt;flood</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.762205</td>\n",
       "      <td>evening&lt;-&gt;night</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.079139</td>\n",
       "      <td>mammal&lt;-&gt;write</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.766022</td>\n",
       "      <td>beef&lt;-&gt;meat</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.076011</td>\n",
       "      <td>portrait&lt;-&gt;rocket</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.767402</td>\n",
       "      <td>car&lt;-&gt;vehicle</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.075713</td>\n",
       "      <td>haircut&lt;-&gt;stop</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.780211</td>\n",
       "      <td>airplane&lt;-&gt;plane</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.073840</td>\n",
       "      <td>abstract&lt;-&gt;rally</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.785247</td>\n",
       "      <td>evening&lt;-&gt;morning</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.072972</td>\n",
       "      <td>canine&lt;-&gt;run</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.793055</td>\n",
       "      <td>summer&lt;-&gt;winter</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.071274</td>\n",
       "      <td>feel&lt;-&gt;stair</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.810065</td>\n",
       "      <td>daughter&lt;-&gt;mother</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.070032</td>\n",
       "      <td>food&lt;-&gt;gull</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.821788</td>\n",
       "      <td>boy&lt;-&gt;girl</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.067949</td>\n",
       "      <td>giraffe&lt;-&gt;harbor</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.829409</td>\n",
       "      <td>bicycle&lt;-&gt;bike</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.067745</td>\n",
       "      <td>lighting&lt;-&gt;panda</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.848555</td>\n",
       "      <td>gold&lt;-&gt;silver</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    min_k_scores            min_k_words  min_k_true  max_k_scores  \\\n",
       "0      -0.123695          carrot<->city         1.0      0.718475   \n",
       "1      -0.116446  construction<->violet         0.6      0.726054   \n",
       "2      -0.109061      jellyfish<->rally         0.4      0.727312   \n",
       "3      -0.107640    bracelet<->mountain         0.8      0.731430   \n",
       "4      -0.102209         people<->stair         4.2      0.737399   \n",
       "5      -0.094024      skating<->sticker         2.2      0.737639   \n",
       "6      -0.093932         lego<->weather         0.8      0.743298   \n",
       "7      -0.090030     daisy<->gravestone         4.6      0.748254   \n",
       "8      -0.088359      origami<->stadium         1.2      0.753521   \n",
       "9      -0.082827     graveyard<->silver         3.0      0.755752   \n",
       "10     -0.081571          clown<->flood         1.2      0.762205   \n",
       "11     -0.079139         mammal<->write         1.0      0.766022   \n",
       "12     -0.076011      portrait<->rocket         2.0      0.767402   \n",
       "13     -0.075713         haircut<->stop         1.4      0.780211   \n",
       "14     -0.073840       abstract<->rally         2.8      0.785247   \n",
       "15     -0.072972           canine<->run         5.2      0.793055   \n",
       "16     -0.071274           feel<->stair         3.4      0.810065   \n",
       "17     -0.070032            food<->gull         4.0      0.821788   \n",
       "18     -0.067949       giraffe<->harbor         0.2      0.829409   \n",
       "19     -0.067745       lighting<->panda         1.8      0.848555   \n",
       "\n",
       "            max_k_words  max_k_true  \n",
       "0        town<->village         8.6  \n",
       "1        dinner<->lunch         7.4  \n",
       "2    breakfast<->dinner         7.4  \n",
       "3        rail<->railway         8.6  \n",
       "4       autumn<->spring         8.8  \n",
       "5   aircraft<->airplane         9.2  \n",
       "6     cafe<->restaurant         8.4  \n",
       "7          shop<->store         7.8  \n",
       "8        daughter<->son         8.2  \n",
       "9        cattle<->sheep         7.6  \n",
       "10      evening<->night         8.4  \n",
       "11          beef<->meat         8.6  \n",
       "12        car<->vehicle         9.2  \n",
       "13     airplane<->plane         8.4  \n",
       "14    evening<->morning         6.4  \n",
       "15      summer<->winter         7.4  \n",
       "16    daughter<->mother         8.0  \n",
       "17           boy<->girl         7.0  \n",
       "18       bicycle<->bike         9.0  \n",
       "19        gold<->silver         8.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame()\n",
    "num_words = 20\n",
    "min_ten = np.argsort(scores_processed)[:num_words]\n",
    "max_ten= np.argsort(scores_processed)[-num_words:]\n",
    "\n",
    "df2[\"min_k_scores\"] = pd.Series(scores_processed[min_ten])\n",
    "df2[\"min_k_words\"] = pd.Series([\"<->\".join((a[0],a[1])) for a in data.X[min_ten]])\n",
    "df2[\"min_k_true\"] = pd.Series([a[0] for a in data.y[min_ten]])\n",
    "df2[\"max_k_scores\"] = pd.Series(scores_processed[max_ten])\n",
    "df2[\"max_k_words\"] = pd.Series([\"<->\".join((a[0],a[1])) for a in data.X[max_ten]])\n",
    "df2[\"max_k_true\"] = pd.Series([a[0] for a in data.y[max_ten]])\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The proccessed vectors improved upon the glove vectors relatively. For eg, aircraft <-> airplane has a true score of 9.2 , processed vectors have a score of 0.737639 > 0.729003 (by glove). \n",
    "- And on few pairs like daughter <-> mother processed vectors have score 0.810065 closer to the true score 8.0 compared to the high score of 0.831060 by glove vectors.\n",
    "- In this way, processed vectors have better overall score on this similarity task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/analogy.py:105: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  A, B, C = np.vstack(w.get(word, mean_vector) for word in X_b[:, 0]), \\\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/analogy.py:106: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  np.vstack(w.get(word, mean_vector) for word in X_b[:, 1]), \\\n",
      "/home/mila/g/gampapha/nlmatics_2/nlmatics/web/analogy.py:107: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  np.vstack(w.get(word, mean_vector) for word in X_b[:, 2])\n"
     ]
    }
   ],
   "source": [
    "from web.analogy import *\n",
    "def evaluate_analogy(w, X, y, method=\"add\", k=None, category=None, batch_size=100):\n",
    "    \"\"\"\n",
    "    Simple method to score embedding using SimpleAnalogySolver\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : Embedding or dict\n",
    "      Embedding or dict instance.\n",
    "\n",
    "    method : {\"add\", \"mul\"}\n",
    "      Method to use when finding analogy answer, see \"Improving Distributional Similarity\n",
    "      with Lessons Learned from Word Embeddings\"\n",
    "\n",
    "    X : array-like, shape (n_samples, 3)\n",
    "      Analogy questions.\n",
    "\n",
    "    y : array-like, shape (n_samples, )\n",
    "      Analogy answers.\n",
    "\n",
    "    k : int, default: None\n",
    "      If not None will select k top most frequent words from embedding\n",
    "\n",
    "    batch_size : int, default: 100\n",
    "      Increase to increase memory consumption and decrease running time\n",
    "\n",
    "    category : list, default: None\n",
    "      Category of each example, if passed function returns accuracy per category\n",
    "      in addition to the overall performance.\n",
    "      Analogy datasets have \"category\" field that can be supplied here.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: dict\n",
    "      Results, where each key is for given category and special empty key \"\" stores\n",
    "      summarized accuracy across categories\n",
    "    \"\"\"\n",
    "    if isinstance(w, dict):\n",
    "        w = Embedding.from_dict(w)\n",
    "\n",
    "    assert category is None or len(category) == y.shape[0], \"Passed incorrect category list\"\n",
    "\n",
    "    solver = SimpleAnalogySolver(w=w, method=method, batch_size=batch_size, k=k)\n",
    "    y_pred = solver.predict(X)\n",
    "\n",
    "    if category is not None:\n",
    "        results = OrderedDict({\"all\": np.mean(y_pred == y)})\n",
    "        count = OrderedDict({\"all\": len(y_pred)})\n",
    "        correct = OrderedDict({\"all\": np.sum(y_pred == y)})\n",
    "        for cat in set(category):\n",
    "            results[cat] = np.mean(y_pred[category == cat] == y[category == cat])\n",
    "            count[cat] = np.sum(category == cat)\n",
    "            correct[cat] = np.sum(y_pred[category == cat] == y[category == cat])\n",
    "\n",
    "        return pd.concat([pd.Series(results, name=\"accuracy\"),\n",
    "                          pd.Series(correct, name=\"correct\"),\n",
    "                          pd.Series(count, name=\"count\")],\n",
    "                         axis=1)\n",
    "    else:\n",
    "        return y_pred, np.mean(y_pred == y)\n",
    "\n",
    "    \n",
    "data= fetch_google_analogy()\n",
    "\n",
    "scores_glove, _ = evaluate_analogy(glove_embeddings,data.X,data.y)\n",
    "scores_processed, _ = evaluate_analogy(processed_embeddings,data.X,data.y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy</th>\n",
       "      <th>True</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>moving&lt;-&gt;moved::hiding&lt;-&gt; ??</td>\n",
       "      <td>hid</td>\n",
       "      <td>fled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weak&lt;-&gt;weaker::small&lt;-&gt; ??</td>\n",
       "      <td>smaller</td>\n",
       "      <td>smaller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vilnius&lt;-&gt;lithuania::ankara&lt;-&gt; ??</td>\n",
       "      <td>turkey</td>\n",
       "      <td>turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sit&lt;-&gt;sits::provide&lt;-&gt; ??</td>\n",
       "      <td>provides</td>\n",
       "      <td>provides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dancing&lt;-&gt;danced::increasing&lt;-&gt; ??</td>\n",
       "      <td>increased</td>\n",
       "      <td>increased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>generate&lt;-&gt;generates::talk&lt;-&gt; ??</td>\n",
       "      <td>talks</td>\n",
       "      <td>radio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fast&lt;-&gt;faster::tall&lt;-&gt; ??</td>\n",
       "      <td>taller</td>\n",
       "      <td>taller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>listen&lt;-&gt;listening::write&lt;-&gt; ??</td>\n",
       "      <td>writing</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hanoi&lt;-&gt;vietnam::bangkok&lt;-&gt; ??</td>\n",
       "      <td>thailand</td>\n",
       "      <td>thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cat&lt;-&gt;cats::lion&lt;-&gt; ??</td>\n",
       "      <td>lions</td>\n",
       "      <td>lions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kabul&lt;-&gt;afghanistan::monrovia&lt;-&gt; ??</td>\n",
       "      <td>liberia</td>\n",
       "      <td>liberia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>belmopan&lt;-&gt;belize::dublin&lt;-&gt; ??</td>\n",
       "      <td>ireland</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nuuk&lt;-&gt;greenland::quito&lt;-&gt; ??</td>\n",
       "      <td>ecuador</td>\n",
       "      <td>ecuador</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nassau&lt;-&gt;bahamas::windhoek&lt;-&gt; ??</td>\n",
       "      <td>namibia</td>\n",
       "      <td>namibia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>possible&lt;-&gt;impossible::ethical&lt;-&gt; ??</td>\n",
       "      <td>unethical</td>\n",
       "      <td>moral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>anaheim&lt;-&gt;california::chandler&lt;-&gt; ??</td>\n",
       "      <td>arizona</td>\n",
       "      <td>calif.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>kathmandu&lt;-&gt;nepal::manama&lt;-&gt; ??</td>\n",
       "      <td>bahrain</td>\n",
       "      <td>bahrain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>germany&lt;-&gt;german::denmark&lt;-&gt; ??</td>\n",
       "      <td>danish</td>\n",
       "      <td>danish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bad&lt;-&gt;worse::deep&lt;-&gt; ??</td>\n",
       "      <td>deeper</td>\n",
       "      <td>deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>brazil&lt;-&gt;real::india&lt;-&gt; ??</td>\n",
       "      <td>rupee</td>\n",
       "      <td>kashmir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>slow&lt;-&gt;slowly::cheerful&lt;-&gt; ??</td>\n",
       "      <td>cheerfully</td>\n",
       "      <td>smiling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>angola&lt;-&gt;kwanza::croatia&lt;-&gt; ??</td>\n",
       "      <td>kuna</td>\n",
       "      <td>croatian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>efficient&lt;-&gt;efficiently::free&lt;-&gt; ??</td>\n",
       "      <td>freely</td>\n",
       "      <td>freely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>chile&lt;-&gt;chilean::spain&lt;-&gt; ??</td>\n",
       "      <td>spanish</td>\n",
       "      <td>spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>boston&lt;-&gt;massachusetts::henderson&lt;-&gt; ??</td>\n",
       "      <td>nevada</td>\n",
       "      <td>gov.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>weird&lt;-&gt;weirdest::dark&lt;-&gt; ??</td>\n",
       "      <td>darkest</td>\n",
       "      <td>darkest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>possible&lt;-&gt;possibly::unfortunate&lt;-&gt; ??</td>\n",
       "      <td>unfortunately</td>\n",
       "      <td>tragic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>running&lt;-&gt;ran::paying&lt;-&gt; ??</td>\n",
       "      <td>paid</td>\n",
       "      <td>paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>akron&lt;-&gt;ohio::austin&lt;-&gt; ??</td>\n",
       "      <td>texas</td>\n",
       "      <td>texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>write&lt;-&gt;writing::see&lt;-&gt; ??</td>\n",
       "      <td>seeing</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Analogy           True  Predicted\n",
       "0              moving<->moved::hiding<-> ??            hid       fled\n",
       "1                weak<->weaker::small<-> ??        smaller    smaller\n",
       "2         vilnius<->lithuania::ankara<-> ??         turkey     turkey\n",
       "3                 sit<->sits::provide<-> ??       provides   provides\n",
       "4        dancing<->danced::increasing<-> ??      increased  increased\n",
       "5          generate<->generates::talk<-> ??          talks      radio\n",
       "6                 fast<->faster::tall<-> ??         taller     taller\n",
       "7           listen<->listening::write<-> ??        writing    writing\n",
       "8            hanoi<->vietnam::bangkok<-> ??       thailand   thailand\n",
       "9                    cat<->cats::lion<-> ??          lions      lions\n",
       "10      kabul<->afghanistan::monrovia<-> ??        liberia    liberia\n",
       "11          belmopan<->belize::dublin<-> ??        ireland    ireland\n",
       "12            nuuk<->greenland::quito<-> ??        ecuador    ecuador\n",
       "13         nassau<->bahamas::windhoek<-> ??        namibia    namibia\n",
       "14     possible<->impossible::ethical<-> ??      unethical      moral\n",
       "15     anaheim<->california::chandler<-> ??        arizona     calif.\n",
       "16          kathmandu<->nepal::manama<-> ??        bahrain    bahrain\n",
       "17          germany<->german::denmark<-> ??         danish     danish\n",
       "18                  bad<->worse::deep<-> ??         deeper     deeper\n",
       "19               brazil<->real::india<-> ??          rupee    kashmir\n",
       "20            slow<->slowly::cheerful<-> ??     cheerfully    smiling\n",
       "21           angola<->kwanza::croatia<-> ??           kuna   croatian\n",
       "22      efficient<->efficiently::free<-> ??         freely     freely\n",
       "23             chile<->chilean::spain<-> ??        spanish    spanish\n",
       "24  boston<->massachusetts::henderson<-> ??         nevada       gov.\n",
       "25             weird<->weirdest::dark<-> ??        darkest    darkest\n",
       "26   possible<->possibly::unfortunate<-> ??  unfortunately     tragic\n",
       "27              running<->ran::paying<-> ??           paid       paid\n",
       "28               akron<->ohio::austin<-> ??          texas      texas\n",
       "29               write<->writing::see<-> ??         seeing          i"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame()\n",
    "num_words = 30\n",
    "indexes = np.random.choice(len(data.X),num_words, replace=False)\n",
    "\n",
    "df1[\"Analogy\"] = pd.Series([ \"::\".join((a[0]+\"<->\"+a[1],a[2] + \"<-> ??\")) for a in data.X[indexes]])\n",
    "df1[\"True\"] = pd.Series(data.y[indexes])\n",
    "df1[\"Predicted\"] = pd.Series(scores_glove[indexes])\n",
    "\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processed Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy</th>\n",
       "      <th>True</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>moving&lt;-&gt;moved::hiding&lt;-&gt; ??</td>\n",
       "      <td>hid</td>\n",
       "      <td>non-institutionalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weak&lt;-&gt;weaker::small&lt;-&gt; ??</td>\n",
       "      <td>smaller</td>\n",
       "      <td>smaller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vilnius&lt;-&gt;lithuania::ankara&lt;-&gt; ??</td>\n",
       "      <td>turkey</td>\n",
       "      <td>turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sit&lt;-&gt;sits::provide&lt;-&gt; ??</td>\n",
       "      <td>provides</td>\n",
       "      <td>provides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dancing&lt;-&gt;danced::increasing&lt;-&gt; ??</td>\n",
       "      <td>increased</td>\n",
       "      <td>increased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>generate&lt;-&gt;generates::talk&lt;-&gt; ??</td>\n",
       "      <td>talks</td>\n",
       "      <td>hears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fast&lt;-&gt;faster::tall&lt;-&gt; ??</td>\n",
       "      <td>taller</td>\n",
       "      <td>taller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>listen&lt;-&gt;listening::write&lt;-&gt; ??</td>\n",
       "      <td>writing</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hanoi&lt;-&gt;vietnam::bangkok&lt;-&gt; ??</td>\n",
       "      <td>thailand</td>\n",
       "      <td>thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cat&lt;-&gt;cats::lion&lt;-&gt; ??</td>\n",
       "      <td>lions</td>\n",
       "      <td>lions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kabul&lt;-&gt;afghanistan::monrovia&lt;-&gt; ??</td>\n",
       "      <td>liberia</td>\n",
       "      <td>liberia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>belmopan&lt;-&gt;belize::dublin&lt;-&gt; ??</td>\n",
       "      <td>ireland</td>\n",
       "      <td>ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nuuk&lt;-&gt;greenland::quito&lt;-&gt; ??</td>\n",
       "      <td>ecuador</td>\n",
       "      <td>ecuador</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nassau&lt;-&gt;bahamas::windhoek&lt;-&gt; ??</td>\n",
       "      <td>namibia</td>\n",
       "      <td>namibia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>possible&lt;-&gt;impossible::ethical&lt;-&gt; ??</td>\n",
       "      <td>unethical</td>\n",
       "      <td>moral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>anaheim&lt;-&gt;california::chandler&lt;-&gt; ??</td>\n",
       "      <td>arizona</td>\n",
       "      <td>nevada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>kathmandu&lt;-&gt;nepal::manama&lt;-&gt; ??</td>\n",
       "      <td>bahrain</td>\n",
       "      <td>bahrain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>germany&lt;-&gt;german::denmark&lt;-&gt; ??</td>\n",
       "      <td>danish</td>\n",
       "      <td>danish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bad&lt;-&gt;worse::deep&lt;-&gt; ??</td>\n",
       "      <td>deeper</td>\n",
       "      <td>deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>brazil&lt;-&gt;real::india&lt;-&gt; ??</td>\n",
       "      <td>rupee</td>\n",
       "      <td>kashmir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>slow&lt;-&gt;slowly::cheerful&lt;-&gt; ??</td>\n",
       "      <td>cheerfully</td>\n",
       "      <td>smiling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>angola&lt;-&gt;kwanza::croatia&lt;-&gt; ??</td>\n",
       "      <td>kuna</td>\n",
       "      <td>croatian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>efficient&lt;-&gt;efficiently::free&lt;-&gt; ??</td>\n",
       "      <td>freely</td>\n",
       "      <td>freely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>chile&lt;-&gt;chilean::spain&lt;-&gt; ??</td>\n",
       "      <td>spanish</td>\n",
       "      <td>spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>boston&lt;-&gt;massachusetts::henderson&lt;-&gt; ??</td>\n",
       "      <td>nevada</td>\n",
       "      <td>nevada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>weird&lt;-&gt;weirdest::dark&lt;-&gt; ??</td>\n",
       "      <td>darkest</td>\n",
       "      <td>darkest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>possible&lt;-&gt;possibly::unfortunate&lt;-&gt; ??</td>\n",
       "      <td>unfortunately</td>\n",
       "      <td>tragic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>running&lt;-&gt;ran::paying&lt;-&gt; ??</td>\n",
       "      <td>paid</td>\n",
       "      <td>paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>akron&lt;-&gt;ohio::austin&lt;-&gt; ??</td>\n",
       "      <td>texas</td>\n",
       "      <td>texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>write&lt;-&gt;writing::see&lt;-&gt; ??</td>\n",
       "      <td>seeing</td>\n",
       "      <td>nohs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Analogy           True  \\\n",
       "0              moving<->moved::hiding<-> ??            hid   \n",
       "1                weak<->weaker::small<-> ??        smaller   \n",
       "2         vilnius<->lithuania::ankara<-> ??         turkey   \n",
       "3                 sit<->sits::provide<-> ??       provides   \n",
       "4        dancing<->danced::increasing<-> ??      increased   \n",
       "5          generate<->generates::talk<-> ??          talks   \n",
       "6                 fast<->faster::tall<-> ??         taller   \n",
       "7           listen<->listening::write<-> ??        writing   \n",
       "8            hanoi<->vietnam::bangkok<-> ??       thailand   \n",
       "9                    cat<->cats::lion<-> ??          lions   \n",
       "10      kabul<->afghanistan::monrovia<-> ??        liberia   \n",
       "11          belmopan<->belize::dublin<-> ??        ireland   \n",
       "12            nuuk<->greenland::quito<-> ??        ecuador   \n",
       "13         nassau<->bahamas::windhoek<-> ??        namibia   \n",
       "14     possible<->impossible::ethical<-> ??      unethical   \n",
       "15     anaheim<->california::chandler<-> ??        arizona   \n",
       "16          kathmandu<->nepal::manama<-> ??        bahrain   \n",
       "17          germany<->german::denmark<-> ??         danish   \n",
       "18                  bad<->worse::deep<-> ??         deeper   \n",
       "19               brazil<->real::india<-> ??          rupee   \n",
       "20            slow<->slowly::cheerful<-> ??     cheerfully   \n",
       "21           angola<->kwanza::croatia<-> ??           kuna   \n",
       "22      efficient<->efficiently::free<-> ??         freely   \n",
       "23             chile<->chilean::spain<-> ??        spanish   \n",
       "24  boston<->massachusetts::henderson<-> ??         nevada   \n",
       "25             weird<->weirdest::dark<-> ??        darkest   \n",
       "26   possible<->possibly::unfortunate<-> ??  unfortunately   \n",
       "27              running<->ran::paying<-> ??           paid   \n",
       "28               akron<->ohio::austin<-> ??          texas   \n",
       "29               write<->writing::see<-> ??         seeing   \n",
       "\n",
       "                Predicted  \n",
       "0   non-institutionalized  \n",
       "1                 smaller  \n",
       "2                  turkey  \n",
       "3                provides  \n",
       "4               increased  \n",
       "5                   hears  \n",
       "6                  taller  \n",
       "7                 writing  \n",
       "8                thailand  \n",
       "9                   lions  \n",
       "10                liberia  \n",
       "11                ireland  \n",
       "12                ecuador  \n",
       "13                namibia  \n",
       "14                  moral  \n",
       "15                 nevada  \n",
       "16                bahrain  \n",
       "17                 danish  \n",
       "18                 deeper  \n",
       "19                kashmir  \n",
       "20                smiling  \n",
       "21               croatian  \n",
       "22                 freely  \n",
       "23                spanish  \n",
       "24                 nevada  \n",
       "25                darkest  \n",
       "26                 tragic  \n",
       "27                   paid  \n",
       "28                  texas  \n",
       "29                   nohs  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame()\n",
    "\n",
    "df2[\"Analogy\"] = pd.Series([ \"::\".join((a[0]+\"<->\"+a[1],a[2] + \"<-> ??\")) for a in data.X[indexes]])\n",
    "df2[\"True\"] = pd.Series(data.y[indexes])\n",
    "df2[\"Predicted\"] = pd.Series(scores_processed[indexes])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlmatics_2)",
   "language": "python",
   "name": "nlmatics_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
